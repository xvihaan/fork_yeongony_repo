{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fd791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "트랜스포머로 한영 번역기 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3472b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "0. 라이브러리 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d2bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "import seaborn\n",
    "\n",
    "print(tensorflow.__version__)\n",
    "print(numpy.__version__)\n",
    "print(matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d0eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. 어텐션 맵 한국어 폰트 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9339e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52142edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. 데이터 정제 및 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02026e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "1) list(set(corpus)) 사용해서 중복 데이터 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf64bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/transformer/data'\n",
    "kor_path = data_dir+\"/korean-english-park.train.ko\"\n",
    "eng_path = data_dir+\"/korean-english-park.train.en\"\n",
    "\n",
    "# 데이터 정제 및 토큰화\n",
    "def clean_corpus(kor_path, eng_path):\n",
    "    with open(kor_path, \"r\") as f: kor = f.read().splitlines()\n",
    "    with open(eng_path, \"r\") as f: eng = f.read().splitlines()\n",
    "    assert len(kor) == len(eng)\n",
    "\n",
    "    # (한국어, 영어) 병렬 데이터로 튜플로 묶기\n",
    "    corpus = list(zip(kor, eng))\n",
    "\n",
    "    # set을 사용하여 중복을 제거하고 리스트로 변환\n",
    "    cleaned_corpus = list(set(corpus))\n",
    "\n",
    "    # 중복을 제거한 병렬 데이터를 다시 분리해서 저장\n",
    "    cleaned_corpus = [(k, e) for k, e in cleaned_corpus]\n",
    "\n",
    "\n",
    "    return cleaned_corpus\n",
    "\n",
    "cleaned_corpus = clean_corpus(kor_path, eng_path)\n",
    "print(cleaned_corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe30be",
   "metadata": {},
   "outputs": [],
   "source": [
    "2) 전처리 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26becad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    # 1. 모든 입력을 소문자로 변환\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # 2. 알파벳, 한글, 문장부호(.,!?가 기준)만 남기고 모두 제거\n",
    "    sentence = re.sub(r\"[^a-zA-Z가-힣0-9.,!?]+\", \" \", sentence)\n",
    "\n",
    "    # 3. 문장부호 양옆에 공백 추가 (.,!?)\n",
    "    sentence = re.sub(r\"([.,!?])\", r\" \\1 \", sentence)\n",
    "\n",
    "    # 4. 문장 앞뒤의 불필요한 공백 제거\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# 테스트 예시\n",
    "test_sentence = \"Hello! 이 문장은 Test 문장입니다. 다른 기호들 #@$도 있어요.\"\n",
    "cleaned_sentence = preprocess_sentence(test_sentence)\n",
    "print(cleaned_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9707041",
   "metadata": {},
   "outputs": [],
   "source": [
    "3) 한글 말뭉치 kor_corpus 와 영문 말뭉치 eng_corpus 를 각각 분리한 후, 정처리 후 SentencePiece로 토큰화를 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e24f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import sentencepiece as spm\n",
    "\n",
    "def generate_tokenizer(corpus, \n",
    "                       vocab_size, \n",
    "                       lang=\"ko\", \n",
    "                       pad_id=0, \n",
    "                       bos_id=1, \n",
    "                       eos_id=2, \n",
    "                       unk_id=3):\n",
    "    # 1. 말뭉치를 텍스트 파일로 저장\n",
    "    temp_file = f\"corpus_{lang}.txt\"\n",
    "    with open(temp_file, 'w', encoding='utf-8') as f:\n",
    "        for line in corpus:\n",
    "            f.write(f\"{line}\\n\")\n",
    "    \n",
    "    # 2. 모델을 저장할 디렉토리가 없으면 생성\n",
    "    model_dir = \"tk_model\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    # 3. SentencePiece 모델 학습\n",
    "    model_prefix = f\"{model_dir}/{lang}_tokenizer\"\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=temp_file,\n",
    "        model_prefix=model_prefix,\n",
    "        vocab_size=vocab_size,\n",
    "        pad_id=pad_id,\n",
    "        bos_id=bos_id,\n",
    "        eos_id=eos_id,\n",
    "        unk_id=unk_id,\n",
    "        model_type='bpe',  # bpe 방식으로 토큰화\n",
    "        user_defined_symbols=[\"<PAD>\", \"<BOS>\", \"<EOS>\", \"<UNK>\"]  # 특수 토큰 설정\n",
    "    )\n",
    "    \n",
    "    # 4. 학습된 SentencePiece 모델을 불러오기\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(f\"{model_prefix}.model\")\n",
    "    \n",
    "    # 5. 불필요한 파일 삭제\n",
    "    os.remove(temp_file)\n",
    "    \n",
    "    return sp\n",
    "\n",
    "# 한국어 및 영어 말뭉치 정제 및 토큰화\n",
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 20000\n",
    "\n",
    "# 예시에서 cleaned_corpus를 분리\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "for pair in cleaned_corpus:\n",
    "    k, e = pair  # cleaned_corpus는 이미 병렬 데이터를 tuple로 저장\n",
    "    kor_corpus.append(preprocess_sentence(k))\n",
    "    eng_corpus.append(preprocess_sentence(e))\n",
    "\n",
    "# 한국어 및 영어 토크나이저 생성\n",
    "ko_tokenizer = generate_tokenizer(kor_corpus, SRC_VOCAB_SIZE, \"ko\")\n",
    "en_tokenizer = generate_tokenizer(eng_corpus, TGT_VOCAB_SIZE, \"en\")\n",
    "\n",
    "# 타겟 토큰에 BOS와 EOS를 포함하도록 설정\n",
    "en_tokenizer.set_encode_extra_options(\"bos:eos\")\n",
    "\n",
    "# 토크나이저 확인\n",
    "print(\"슝=3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846b6693",
   "metadata": {},
   "outputs": [],
   "source": [
    "4) 토크나이저를 활용해 토큰의 길이가 50 이하인 데이터를 선별하여 src_corpus 와 tgt_corpus 를 각각 구축하고, 텐서 enc_train 과 dec_train 으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf34722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm  # Progress Bar 보기 위해 사용\n",
    "import tensorflow as tf\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "assert len(kor_corpus) == len(eng_corpus)  # 한국어와 영어 문장의 개수가 일치하는지 확인\n",
    "\n",
    "# 토큰의 길이가 50 이하인 문장만 남깁니다.\n",
    "for idx in tqdm(range(len(kor_corpus))):\n",
    "    # 한국어 문장 토큰화\n",
    "    src_tokens = ko_tokenizer.encode(kor_corpus[idx], out_type=int)\n",
    "    \n",
    "    # 영어 문장 토큰화 (BOS와 EOS 포함)\n",
    "    tgt_tokens = en_tokenizer.encode(eng_corpus[idx], out_type=int)\n",
    "    \n",
    "    # 토큰 길이가 50 이하인 경우만 추가\n",
    "    if len(src_tokens) <= 50 and len(tgt_tokens) <= 50:\n",
    "        src_corpus.append(src_tokens)\n",
    "        tgt_corpus.append(tgt_tokens)\n",
    "\n",
    "# 패딩 처리를 완료하여 학습용 데이터를 완성합니다.\n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')\n",
    "\n",
    "# enc_train과 dec_train의 형태 출력\n",
    "print(f\"enc_train shape: {enc_train.shape}\")\n",
    "print(f\"dec_train shape: {dec_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3979ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d31bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    return sinusoid_table\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32580353",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "            \n",
    "        self.depth = d_model // self.num_heads\n",
    "            \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "            \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "        # Q: [batch_size, num_heads, seq_len_q, depth_q]\n",
    "        # K: [batch_size, num_heads, seq_len_k, depth_k]\n",
    "        # V: [batch_size, num_heads, seq_len_k, depth_v]\n",
    "\n",
    "        # Q, K, V의 shape 출력\n",
    "        print(f\"Q shape: {Q.shape}\")\n",
    "        print(f\"K shape: {K.shape}\")\n",
    "        print(f\"V shape: {V.shape}\")\n",
    "\n",
    "        # 마스크의 shape 출력\n",
    "        if mask is not None:\n",
    "            print(f\"mask shape: {mask.shape}\")\n",
    "\n",
    "        # Q와 K의 매트릭스 곱셈\n",
    "        matmul_qk = tf.matmul(Q, K, transpose_b=True)  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "        # Scale the matmul_qk\n",
    "        dk = tf.cast(tf.shape(K)[-1], tf.float32)\n",
    "        scaled_qk = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        # Apply mask (if provided)\n",
    "        if mask is not None:\n",
    "            # 마스크와 scaled_qk의 shape이 맞는지 확인\n",
    "            print(f\"scaled_qk shape: {scaled_qk.shape}\")\n",
    "            print(f\"mask shape (before applying): {mask.shape}\")\n",
    "\n",
    "            # 마스크 추가 (shape이 맞는지 확인하고, 맞지 않으면 reshape 필요)\n",
    "            scaled_qk += (mask * -1e9)\n",
    "\n",
    "        # Softmax on the last axis (seq_len_k)\n",
    "        attention_weights = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "\n",
    "        # Multiply attention weights with V\n",
    "        output = tf.matmul(attention_weights, V)  # (batch_size, num_heads, seq_len_q, depth_v)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "        \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "            \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "    \t\t\t\t        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "                \n",
    "        return out, attention_weights\n",
    "    \n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcf4a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "            \n",
    "        self.depth = d_model // self.num_heads\n",
    "            \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "            \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Perform scaled dot-product attention with mask.\n",
    "        \"\"\"\n",
    "        # Matmul of Q and K\n",
    "        matmul_qk = tf.matmul(Q, K, transpose_b=True)  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "        # Scale matmul_qk\n",
    "        dk = tf.cast(tf.shape(K)[-1], tf.float32)\n",
    "        scaled_qk = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        # Apply mask (if provided)\n",
    "        if mask is not None:\n",
    "            scaled_qk += (mask * -1e9)\n",
    "\n",
    "        # Softmax over the last axis (seq_len_k)\n",
    "        attention_weights = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "\n",
    "        # Matmul of attention weights and V\n",
    "        output = tf.matmul(attention_weights, V)  # (batch_size, num_heads, seq_len_q, depth_v)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):  # self 추가\n",
    "        # Q: [batch_size, num_heads, seq_len_q, depth_q]\n",
    "        # K: [batch_size, num_heads, seq_len_k, depth_k]\n",
    "        # V: [batch_size, num_heads, seq_len_k, depth_v]\n",
    "\n",
    "        # Q, K, V의 shape 출력\n",
    "        print(f\"Q shape: {Q.shape}\")\n",
    "        print(f\"K shape: {K.shape}\")\n",
    "        print(f\"V shape: {V.shape}\")\n",
    "\n",
    "        # 마스크의 shape 출력\n",
    "        if mask is not None:\n",
    "            print(f\"mask shape: {mask.shape}\")\n",
    "\n",
    "        # Q와 K의 매트릭스 곱셈\n",
    "        matmul_qk = tf.matmul(Q, K, transpose_b=True)  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "        # Scale the matmul_qk\n",
    "        dk = tf.cast(tf.shape(K)[-1], tf.float32)\n",
    "        scaled_qk = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        # Apply mask (if provided)\n",
    "        if mask is not None:\n",
    "            # 마스크와 scaled_qk의 shape이 맞는지 확인\n",
    "            print(f\"scaled_qk shape: {scaled_qk.shape}\")\n",
    "            print(f\"mask shape (before applying): {mask.shape}\")\n",
    "\n",
    "            # 마스크 추가 (shape이 맞는지 확인하고, 맞지 않으면 reshape 필요)\n",
    "            scaled_qk += (mask * -1e9)\n",
    "\n",
    "        # Softmax on the last axis (seq_len_k)\n",
    "        attention_weights = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "\n",
    "        # Multiply attention weights with V\n",
    "        output = tf.matmul(attention_weights, V)  # (batch_size, num_heads, seq_len_q, depth_v)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "        \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "            \n",
    "        # scaled_dot_product_attention 호출 시 정확한 4개의 인자 전달\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "    \t\t\t\t        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "                \n",
    "        return out, attention_weights\n",
    "\n",
    "print(\"슝=3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7287ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df0010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5aa042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention (Self-Attention)\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, causality_mask)  # Self-Attention에서는 causality_mask 사용\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Encoder-Decoder Cross Attention (No Causality Mask, Only Padding Mask)\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, padding_mask)  # padding_mask만 사용\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn\n",
    "\n",
    "print(\"슝=3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ee2a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1480859",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            # 수정: DecoderLayer의 call 함수에서 cross attention에서는 padding_mask만 전달\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "\n",
    "print(\"슝=3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf34ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        # 수정: 디코더에서 cross-attention에는 dec_mask만 전달\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)  # dec_mask는 padding_mask로 전달\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "print(\"슝=3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e571d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)  # 크기만을 받도록 수정\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    # 인코더 패딩 마스크 (src 시퀀스에 대해)\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    # 디코더의 패딩 마스크 (tgt 시퀀스에 대해)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "    \n",
    "    # 디코더의 자기 자신에 대한 인과 마스크 (룩어헤드 마스크)\n",
    "    causality_mask = generate_causality_mask(tf.shape(tgt)[1])\n",
    "    combined_dec_mask = tf.maximum(dec_mask, causality_mask)\n",
    "\n",
    "    # 인코더-디코더 크로스 어텐션을 위한 패딩 마스크 (src 시퀀스에 대해)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, combined_dec_mask\n",
    "\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469a5f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79939df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 하이퍼파라미터 설정\n",
    "n_layers = 2          # 트랜스포머 레이어 수\n",
    "d_model = 512         # 모델 차원\n",
    "n_heads = 8           # 멀티 헤드 어텐션에서의 헤드 수\n",
    "d_ff = 2048           # 피드 포워드 네트워크의 차원\n",
    "src_vocab_size = 20000 # 소스 언어의 단어 사전 크기\n",
    "tgt_vocab_size = 20000 # 타겟 언어의 단어 사전 크기\n",
    "pos_len = 100         # 포지션 인코딩의 길이\n",
    "dropout = 0.1         # 드롭아웃 비율\n",
    "\n",
    "# 2 Layer를 가지는 Transformer 모델 선언\n",
    "transformer = Transformer(n_layers=n_layers,\n",
    "                          d_model=d_model,\n",
    "                          n_heads=n_heads,\n",
    "                          d_ff=d_ff,\n",
    "                          src_vocab_size=src_vocab_size,\n",
    "                          tgt_vocab_size=tgt_vocab_size,\n",
    "                          pos_len=pos_len,\n",
    "                          dropout=dropout)\n",
    "\n",
    "print(\"Transformer 모델 생성 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c96e5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 논문에서 사용된 Learning Rate Scheduler 정의\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5  # step의 역수의 제곱근\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)  # warmup_steps의 제곱근을 곱한 값\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "# 모델의 차원(d_model)에 따라 학습률 스케줄러 생성\n",
    "learning_rate = LearningRateScheduler(d_model=512)\n",
    "\n",
    "# Adam Optimizer 선언 (논문에서 사용된 파라미터와 동일하게 설정)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "print(\"슝=3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c450009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7342eae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]  # 타겟의 실제값에서 <BOS> 토큰을 제외한 부분\n",
    "    \n",
    "    # 마스크 생성\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # 계산된 loss에 tf.GradientTape() 적용하여 학습 진행\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # 그라디언트 계산 및 적용\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8cf004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcafb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 함수\n",
    "\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165a2762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 및 Attention 시각화 결합\n",
    "\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451ad070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1\n",
    "\n",
    "examples = [\n",
    "            \"오바마는 대통령이다.\",\n",
    "            \"시민들은 도시 속에 산다.\",\n",
    "            \"커피는 필요 없다.\",\n",
    "            \"일곱 명의 사망자가 발생했다.\"\n",
    "]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "\n",
    "    for example in examples:\n",
    "        translate(example, transformer, ko_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf7ad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "회고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a5f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "트랜스포머 학습을 하면서 가장 깊게 공부할 수 있었던 것 같다. 노드의 도입부 대로 차원을 확인하면서 공부하니 각 함수의 의미와 흐름, 그리고 코드에서 차원의 크기에 문제가 생겼을 때도 덜 겁을 먹고 코드를 뜯어보며 해결하려고 노력했던 것 같다. 다만, 트랜스포머 모듈? 구조에 오랜 시간을 써서 뒷부분인 손실함수, 학습률 등은 사실 자세히 못봐서 아쉽다. 다음에 또 이에 대해 학습할 기회가 있으면 좋을 것 같당!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
