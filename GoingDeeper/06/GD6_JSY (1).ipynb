{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f81af05",
   "metadata": {},
   "source": [
    "# 멋진 챗봇 만들기 프로젝트 (트랜스포머 구조, 동의어 데이터 증강, BELU 스코어 사용해보기)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a963e64e",
   "metadata": {},
   "source": [
    "데이터 증강 모델 가져올 때, 구버전 사용하기 (by.승환님)\n",
    "https://iambeginnerdeveloper.tistory.com/41"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1660c288",
   "metadata": {},
   "source": [
    "## 0. 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "82bfa6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "import gensim\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b1b89f",
   "metadata": {},
   "source": [
    "## 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c548c9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Question: 12시 땡!\n",
      "Sample Answer: 하루가 또 가네요.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 경로와 파일 이름\n",
    "file_path = '~/aiffel/Chapter18/ChatbotData.csv'\n",
    "\n",
    "# CSV 파일을 읽어오기\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# questions와 answers 컬럼을 각각 리스트로 저장\n",
    "questions = data['Q'].tolist()\n",
    "answers = data['A'].tolist()\n",
    "\n",
    "print(\"Sample Question:\", questions[0])\n",
    "print(\"Sample Answer:\", answers[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02667c8",
   "metadata": {},
   "source": [
    "## 2. 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "6bbfb522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문: Hello, AIFFEL! Let's learn about NLP. #AI #2023\n",
      "전처리 후: hello, aiffel! lets learn about nlp. ai 2023\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r\"[^a-zA-Z0-9가-힣\\s.,!?]\", \"\", sentence)\n",
    "    return sentence\n",
    "\n",
    "sample_sentence = \"Hello, AIFFEL! Let's learn about NLP. #AI #2023\"\n",
    "processed_sentence = preprocess_sentence(sample_sentence)\n",
    "print(\"원문:\", sample_sentence)\n",
    "print(\"전처리 후:\", processed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acd8a2f",
   "metadata": {},
   "source": [
    "## 3. 데이터 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85591c5",
   "metadata": {},
   "source": [
    "1. 소스 문장 데이터와 타겟 문장 데이터를 입력으로 받습니다.\n",
    "2. 데이터를 앞서 정의한 preprocess_sentence() 함수로 정제하고, 토큰화합니다.\n",
    "3. 토큰화는 전달받은 토크나이즈 함수를 사용합니다. 이번엔 mecab.morphs 함수를 전달하시면 됩니다.\n",
    "4. 토큰의 개수가 일정 길이 이상인 문장은 데이터에서 제외합니다.\n",
    "5. 중복되는 문장은 데이터에서 제외합니다. 소스 : 타겟 쌍을 비교하지 않고 소스는 소스대로 타겟은 타겟대로 검사합니다. 중복 쌍이 흐트러지지 않도록 유의하세요!\n",
    "\n",
    "구현한 함수를 활용하여 questions 와 answers 를 각각 que_corpus , ans_corpus 에 토큰화하여 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e2c5eb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions Average Length: 6.992157876594278\n",
      "Answers Average Length: 8.530188186095138\n",
      "Examples from que_corpus:\n",
      "1: ['12', '시', '땡', '!']\n",
      "2: ['1', '지망', '학교', '떨어졌', '어']\n",
      "3: ['3', '박', '4', '일', '놀', '러', '가', '고', '싶', '다']\n",
      "4: ['3', '박', '4', '일', '정도', '놀', '러', '가', '고', '싶', '다']\n",
      "5: ['ppl', '심하', '네']\n",
      "Examples from ans_corpus:\n",
      "1: ['하루', '가', '또', '가', '네요', '.']\n",
      "2: ['위로', '해', '드립니다', '.']\n",
      "3: ['여행', '은', '언제나', '좋', '죠', '.']\n",
      "4: ['눈살', '이', '찌푸려', '지', '죠', '.']\n",
      "5: ['다시', '새로', '사', '는', '게', '마음', '편해요', '.']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    # 예시 정제 함수 (구체적인 구현은 필요시 추가)\n",
    "    return sentence.lower()  # 간단히 소문자로 변환한다고 가정\n",
    "\n",
    "def build_corpus(sentences, tokenizer, max_len=20):\n",
    "    \"\"\"\n",
    "    주어진 문장 데이터를 정제하고 토큰화하여 길이와 중복 조건을 만족하는 코퍼스를 생성합니다.\n",
    "    \n",
    "    Parameters:\n",
    "    - sentences (list of str): 입력 문장 데이터 리스트 (예: questions, answers)\n",
    "    - tokenizer (function): 토큰화 함수 (예: mecab.morphs)\n",
    "    - max_len (int): 허용하는 최대 토큰 개수 (기본값: 20)\n",
    "    \n",
    "    Returns:\n",
    "    - list of list of str: 정제 및 토큰화된 코퍼스 데이터\n",
    "    - list of int: 각 문장의 토큰 길이\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    lengths = []  # 각 문장의 토큰 길이 저장용\n",
    "    seen_sentences = set()  # 중복 제거를 위한 집합\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # 1. 문장 정제\n",
    "        clean_sentence = preprocess_sentence(sentence)\n",
    "        \n",
    "        # 2. 문장 토큰화\n",
    "        tokenized_sentence = tokenizer(clean_sentence)\n",
    "        \n",
    "        # 3. 최대 길이 필터링\n",
    "        if len(tokenized_sentence) > max_len:\n",
    "            continue\n",
    "        \n",
    "        # 4. 중복 문장 제거\n",
    "        tokenized_str = ' '.join(tokenized_sentence)\n",
    "        if tokenized_str not in seen_sentences:\n",
    "            seen_sentences.add(tokenized_str)\n",
    "            corpus.append(tokenized_sentence)\n",
    "            lengths.append(len(tokenized_sentence))\n",
    "    \n",
    "    return corpus, lengths\n",
    "\n",
    "que_corpus, que_lengths = build_corpus(questions, mecab.morphs)\n",
    "ans_corpus, ans_lengths = build_corpus(answers, mecab.morphs)\n",
    "\n",
    "# 각 열의 평균 길이 계산\n",
    "que_avg_len = np.mean(que_lengths)\n",
    "ans_avg_len = np.mean(ans_lengths)\n",
    "\n",
    "print(\"Questions Average Length:\", que_avg_len)\n",
    "print(\"Answers Average Length:\", ans_avg_len)\n",
    "\n",
    "# ans_corpus에서 5개의 예시 출력\n",
    "print(\"Examples from que_corpus:\")\n",
    "for i, example in enumerate(que_corpus[:5]):\n",
    "    print(f\"{i + 1}: {example}\")\n",
    "print(\"Examples from ans_corpus:\")\n",
    "for i, example in enumerate(ans_corpus[:5]):\n",
    "    print(f\"{i + 1}: {example}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce6389d",
   "metadata": {},
   "source": [
    "## 4. 데이터 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4ab98f1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==3.8.3 in /opt/conda/lib/python3.9/site-packages (3.8.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (5.2.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.16.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.21.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim==3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "583b4e46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'Vocab' on <module 'gensim.models.word2vec' from '/opt/conda/lib/python3.9/site-packages/gensim/models/word2vec.py'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33/506429002.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Word2Vec 모델 로드 (ko.bin 파일이 위치한 경로로 변경)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'~/aiffel/ko.bin'\u001b[0m  \u001b[0;31m# 업로드한 파일 경로로 지정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mwv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"모델 로드 완료!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattrib\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__scipys'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading %s from %s with mmap=%s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             \u001b[0msparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1458\u001b[0m                     \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1460\u001b[0;31m                 )\n\u001b[0m\u001b[1;32m   1461\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'Vocab' on <module 'gensim.models.word2vec' from '/opt/conda/lib/python3.9/site-packages/gensim/models/word2vec.py'>"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Word2Vec 모델 로드 (ko.bin 파일이 위치한 경로로 변경)\n",
    "model_path = '~/aiffel/ko.bin'  # 업로드한 파일 경로로 지정\n",
    "wv = KeyedVectors.load(model_path)\n",
    "print(\"모델 로드 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "02468574",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'Vocab' on <module 'gensim.models.word2vec' from '/opt/conda/lib/python3.9/site-packages/gensim/models/word2vec.py'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33/2460556186.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Word2Vec 모델 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mwv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"모델 로드 완료!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, rethrow, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, rethrow, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattrib\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__scipys'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading %s from %s with mmap=%s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             \u001b[0msparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1458\u001b[0m                     \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1460\u001b[0;31m                 )\n\u001b[0m\u001b[1;32m   1461\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'Vocab' on <module 'gensim.models.word2vec' from '/opt/conda/lib/python3.9/site-packages/gensim/models/word2vec.py'>"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# ko.bin 파일 경로 설정\n",
    "model_path = '~/aiffel/ko.bin'\n",
    "\n",
    "# Word2Vec 모델 로드\n",
    "wv = Word2Vec.load(model_path)\n",
    "print(\"모델 로드 완료!\")\n",
    "\n",
    "# 모델 테스트 예시\n",
    "print(wv.most_similar(\"학교\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1f88f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def lexical_sub(sentence, wv, substitution_prob=0.3):\n",
    "    \"\"\"\n",
    "    입력 문장을 Embedding 유사도를 기반으로 일부 단어를 대체하여 데이터 증강을 수행합니다.\n",
    "    \n",
    "    Parameters:\n",
    "    - sentence (str): 입력 문장\n",
    "    - wv (KeyedVectors): 사전 훈련된 Word2Vec 모델\n",
    "    - substitution_prob (float): 각 단어를 대체할 확률 (기본값: 0.3)\n",
    "    \n",
    "    Returns:\n",
    "    - str: 대체된 문장\n",
    "    \"\"\"\n",
    "    tokens = sentence.split()\n",
    "    result = []\n",
    "\n",
    "    for tok in tokens:\n",
    "        # substitution_prob에 따라 단어를 유사한 단어로 대체\n",
    "        if random.random() < substitution_prob:\n",
    "            try:\n",
    "                # 가장 유사한 단어를 찾고 치환\n",
    "                similar_word = wv.most_similar(tok)[0][0]\n",
    "                result.append(similar_word)\n",
    "            except KeyError:\n",
    "                # 모델에 단어가 없으면 원래 단어 그대로 사용\n",
    "                result.append(tok)\n",
    "        else:\n",
    "            # 대체하지 않는 경우 원래 단어 그대로 사용\n",
    "            result.append(tok)\n",
    "    \n",
    "    return ' '.join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b562591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Augmentation된 데이터 저장할 리스트\n",
    "augmented_que_corpus = []\n",
    "augmented_ans_corpus = []\n",
    "\n",
    "# que_corpus의 Augmentation과 ans_corpus의 원본을 병렬 데이터로 생성\n",
    "for sentence in tqdm(que_corpus):\n",
    "    augmented_sentence = lexical_sub(' '.join(sentence), wv)\n",
    "    augmented_que_corpus.append(augmented_sentence.split())  # 토큰화된 형태로 저장\n",
    "\n",
    "# ans_corpus의 Augmentation과 que_corpus의 원본을 병렬 데이터로 생성\n",
    "for sentence in tqdm(ans_corpus):\n",
    "    augmented_sentence = lexical_sub(' '.join(sentence), wv)\n",
    "    augmented_ans_corpus.append(augmented_sentence.split())  # 토큰화된 형태로 저장\n",
    "\n",
    "# 최종 데이터를 합쳐서 3배로 확장\n",
    "final_que_corpus = que_corpus + augmented_que_corpus\n",
    "final_ans_corpus = ans_corpus + ans_corpus  # que_corpus의 Augmentation과 원본 ans_corpus의 병렬 데이터\n",
    "final_que_corpus += que_corpus  # ans_corpus의 Augmentation과 원본 que_corpus의 병렬 데이터\n",
    "final_ans_corpus += augmented_ans_corpus\n",
    "\n",
    "print(\"Original corpus size:\", len(que_corpus))\n",
    "print(\"Augmented corpus size:\", len(final_que_corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f61ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "2e1ce351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 examples from ans_corpus with start/end tokens:\n",
      "['<start>', '하루', '가', '또', '가', '네요', '.', '<end>']\n",
      "['<start>', '위로', '해', '드립니다', '.', '<end>']\n",
      "['<start>', '여행', '은', '언제나', '좋', '죠', '.', '<end>']\n"
     ]
    }
   ],
   "source": [
    "# ans_corpus에 <start>와 <end> 토큰 추가\n",
    "ans_corpus = [[\"<start>\"] + sentence + [\"<end>\"] for sentence in ans_corpus]\n",
    "\n",
    "# 예시 출력 (첫 3개 문장 확인)\n",
    "print(\"First 3 examples from ans_corpus with start/end tokens:\")\n",
    "for i in range(3):\n",
    "    print(ans_corpus[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d0842fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 사전에 포함된 단어 수: 6822\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 질문과 답변 데이터를 결합하여 단어 사전 구축\n",
    "total_corpus = que_corpus + ans_corpus\n",
    "\n",
    "# Tokenizer 생성 (필요시 num_words로 어휘 크기 제한)\n",
    "tokenizer = Tokenizer(filters='', oov_token='<unk>')\n",
    "tokenizer.fit_on_texts([' '.join(sentence) for sentence in total_corpus])\n",
    "\n",
    "# 단어 사전 정보 출력\n",
    "print(f\"단어 사전에 포함된 단어 수: {len(tokenizer.word_index)}\")\n",
    "\n",
    "# 단어 사전 크기 확인\n",
    "vocab_size = len(tokenizer.word_index) + 1  # 패딩 토큰 포함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8839b73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample encoded question: [2054, 229, 3004, 106]\n",
      "Sample encoded answer: [3, 241, 8, 132, 8, 47, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "# 질문 데이터(que_corpus)를 시퀀스로 변환\n",
    "enc_train = tokenizer.texts_to_sequences([' '.join(sentence) for sentence in que_corpus])\n",
    "\n",
    "# 답변 데이터(ans_corpus)를 시퀀스로 변환\n",
    "dec_train = tokenizer.texts_to_sequences([' '.join(sentence) for sentence in ans_corpus])\n",
    "\n",
    "# 벡터화된 데이터 예시 출력\n",
    "print(\"Sample encoded question:\", enc_train[0])\n",
    "print(\"Sample encoded answer:\", dec_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f73920cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of enc_train: (11604, 20)\n",
      "Shape of dec_train: (7652, 22)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 패딩 처리 (필요시 maxlen으로 최대 길이 지정)\n",
    "enc_train = pad_sequences(enc_train, padding='post')\n",
    "dec_train = pad_sequences(dec_train, padding='post')\n",
    "\n",
    "print(\"Shape of enc_train:\", enc_train.shape)\n",
    "print(\"Shape of dec_train:\", dec_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebd515c",
   "metadata": {},
   "source": [
    "## 오류\n",
    "차원이 안맞는 오류가 발생하여 dec 크기에 맞춰 enc도 잘라주었음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "096d2a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Shape of enc_train: (7652, 20)\n",
      "Updated Shape of dec_train: (7652, 22)\n"
     ]
    }
   ],
   "source": [
    "# 두 데이터의 최소 길이에 맞춰 슬라이싱\n",
    "min_len = min(len(enc_train), len(dec_train))\n",
    "enc_train = enc_train[:min_len]\n",
    "dec_train = dec_train[:min_len]\n",
    "\n",
    "print(\"Updated Shape of enc_train:\", enc_train.shape)\n",
    "print(\"Updated Shape of dec_train:\", dec_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "2f022a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Dataset 생성 및 셔플링, 배치 처리\n",
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "bd97c27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "86bc0e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_lookahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "180ee064",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "3c7eab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b87b0100",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8ce9efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        # Q, K, V 순서에 주의하세요!\n",
    "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e3ff0065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d4f47823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "f547fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "197c38d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\t\t\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "937f9ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "664e4f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "9c1fb3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "f06dc677",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "b0a2e5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 함수\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer, max_len=50):\n",
    "    sentence = preprocess_sentence(sentence)  # 전처리 함수 사용\n",
    "    tokens = src_tokenizer.texts_to_sequences([sentence])[0]  # 토큰화\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens], maxlen=max_len, padding='post')\n",
    "\n",
    "    output = tf.expand_dims([tgt_tokenizer.word_index['<start>']], 0)\n",
    "    ids = []\n",
    "\n",
    "    for i in range(max_len):\n",
    "        predictions, _, _, _ = model(_input, output, None, None, None)\n",
    "        predicted_id = tf.argmax(predictions[0, -1]).numpy()\n",
    "\n",
    "        if predicted_id == tgt_tokenizer.word_index['<end>']:\n",
    "            break\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    return ' '.join([tgt_tokenizer.index_word[id] for id in ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3be622c",
   "metadata": {},
   "source": [
    "훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "8b5fe22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 119/119 [00:10<00:00, 11.49batch/s, Loss=5.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.7688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 119/119 [00:06<00:00, 17.61batch/s, Loss=3.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.8294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 119/119 [00:06<00:00, 17.54batch/s, Loss=3.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.0323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 119/119 [00:06<00:00, 17.41batch/s, Loss=2.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 2.2424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 119/119 [00:06<00:00, 17.35batch/s, Loss=1.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 1.5562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 119/119 [00:06<00:00, 17.32batch/s, Loss=1.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 1.2901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 119/119 [00:06<00:00, 17.32batch/s, Loss=1.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 1.2322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 119/119 [00:06<00:00, 17.37batch/s, Loss=1.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 1.2678\n",
      "\n",
      "질문: 지루하다, 놀러가고 싶어.\n",
      "예측 답변: 사람 마다 다르 지 않 는 게 좋 아요 .\n",
      "\n",
      "질문: 오늘 일찍 일어났더니 피곤하다.\n",
      "예측 답변: 천천히 그만두 세요 .\n",
      "\n",
      "질문: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "예측 답변: 나쁜 마음 이 없 어서 좋 겠 어요 .\n",
      "\n",
      "질문: 집에 있는다는 소리야.\n",
      "예측 답변: 사람 마다 다르 지 않 는 게 좋 아요 .\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "EPOCHS = 8\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = len(questions)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # 단어 사전 크기\n",
    "\n",
    "# 1. Dataset 생성\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# 모델 및 옵티마이저 초기화\n",
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True\n",
    ")\n",
    "learning_rate = LearningRateScheduler(512, warmup_steps=1000)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# 2. 훈련 루프\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    dataset_count = tf.data.experimental.cardinality(dataset).numpy()\n",
    "    tqdm_bar = tqdm(total=dataset_count, desc=f'Epoch {epoch + 1}', unit='batch')\n",
    "\n",
    "    for (batch, (src, tgt)) in enumerate(dataset):\n",
    "        loss, enc_attns, dec_attns, dec_enc_attns = train_step(src, tgt, transformer, optimizer)\n",
    "        total_loss += loss\n",
    "        tqdm_bar.set_postfix({'Loss': total_loss.numpy() / (batch + 1)})\n",
    "        tqdm_bar.update(1)\n",
    "    \n",
    "    tqdm_bar.close()\n",
    "    print(f'Epoch {epoch + 1} Loss: {total_loss.numpy() / dataset_count:.4f}')\n",
    "\n",
    "# 예문 번역 생성\n",
    "test_questions = [\n",
    "    \"지루하다, 놀러가고 싶어.\",\n",
    "    \"오늘 일찍 일어났더니 피곤하다.\",\n",
    "    \"간만에 여자친구랑 데이트 하기로 했어.\",\n",
    "    \"집에 있는다는 소리야.\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n질문: {question}\")\n",
    "    predicted_answer = evaluate(question, transformer, tokenizer, tokenizer)\n",
    "    print(f\"예측 답변: {predicted_answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "3de21ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    \"\"\"\n",
    "    BLEU 점수를 계산합니다.\n",
    "    \n",
    "    Parameters:\n",
    "    - reference (list of str): 실제 정답 문장 (리스트 형태로)\n",
    "    - candidate (list of str): 모델이 예측한 답변 (리스트 형태로)\n",
    "    \n",
    "    Returns:\n",
    "    - float: 계산된 BLEU 점수\n",
    "    \"\"\"\n",
    "    # BLEU 계산 시, smoothing function을 사용해 0 점수를 방지합니다.\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    score = sentence_bleu([reference], candidate, smoothing_function=smoothie)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "0ea9a253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.2118\n"
     ]
    }
   ],
   "source": [
    "# 예시 질문과 모델 예측 답변, 실제 답변 (여기서는 예시를 사용)\n",
    "sample_question = \"오늘 날씨 어때?\"\n",
    "sample_reference = [\"오늘\", \"날씨\", \"맑아요\"]\n",
    "sample_candidate = [\"오늘\", \"날씨\", \"좋아요\"]  # 모델의 예측 결과 예시\n",
    "\n",
    "# BLEU 점수 계산\n",
    "bleu_score = calculate_bleu(sample_reference, sample_candidate)\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "7ee4f822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# 예시 질문과 모델 예측 답변, 실제 답변 (여기서는 예시를 사용)\n",
    "sample_question = \"오늘 일찍 일어났더니 피곤하다.\"\n",
    "sample_reference = [\"오늘\", \"일찍\", \"주무세요\"]\n",
    "sample_candidate = [\"천천히\", \"그만두\", \"세요\"]  # 모델의 예측 결과 예시\n",
    "\n",
    "# BLEU 점수 계산\n",
    "bleu_score = calculate_bleu(sample_reference, sample_candidate)\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7b75c0",
   "metadata": {},
   "source": [
    "회고\n",
    "\n",
    "트랜스포머의 구조와 학습속도, 손실함수를 한번 더 톺아볼 수 있었다.\n",
    "데이터 증강에서 버전을 다운그레이드 했는데도 해결이 안되어서 궁금하다.\n",
    "챗봇의 경우, 번역보다는 belu 스코어를 활용하기 어려운 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cddc81f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
