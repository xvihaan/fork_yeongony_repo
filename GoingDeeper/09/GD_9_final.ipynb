{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd183181",
   "metadata": {},
   "source": [
    "# KoChatGPT 업그레이드 하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa82b4b2",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "390e10e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:1.12.1\n",
      "Cuda version: 11.3\n",
      "transformers version: 4.28.0\n",
      "GPU 사용 가능여부: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"Torch version:{}\".format(torch.__version__)) # Torch version:1.12.1\n",
    "print(\"Cuda version: {}\".format(torch.version.cuda)) # Cuda version: 11.3\n",
    "print(\"transformers version: {}\".format(transformers.__version__)) # transformers 4.28.0\n",
    "print(\"GPU 사용 가능여부: {}\".format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f1a1f",
   "metadata": {},
   "source": [
    "## 1. foundation model 변경하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a19cf447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33cea701a744fafbbceac7cc1760618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ccae98e2d70494482f53f70aa1c0fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/731 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49d00fee43b476a964468830aba026f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d27cdb92af44ffe9f1c37215f2e41b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/109 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "127b0a7419c1416db1da73f340754dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/4.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"skt/ko-gpt-trinity-1.2B-v0.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f0d81c",
   "metadata": {},
   "source": [
    "우리가 사용할 모델의 토크나이저가 입력받아 처리할 수 있는 최대 토큰 수를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "536bcd0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt2': 1024,\n",
       " 'gpt2-medium': 1024,\n",
       " 'gpt2-large': 1024,\n",
       " 'gpt2-xl': 1024,\n",
       " 'distilgpt2': 1024}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9419be5e",
   "metadata": {},
   "source": [
    "gpt2 (small): 가장 작은 버전으로, 117M(1억 1천7백만)개의 파라미터를 가지고 있습니다. <br>\n",
    "gpt2-medium: 345M(3억 4천5백만)개의 파라미터를 가진 중간 크기 모델입니다.<br>\n",
    "gpt2-large: 774M(7억 7천4백만)개의 파라미터를 가진 큰 모델입니다.<br>\n",
    "gpt2-xl: GPT-2 시리즈 중 가장 큰 모델로, 1.5B(15억)개의 파라미터를 가지고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ace29",
   "metadata": {},
   "source": [
    "토크나이징 방식 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bdd9242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kogpt-2_tokens</th>\n",
       "      <td>▁바람</td>\n",
       "      <td>도</td>\n",
       "      <td>▁없는</td>\n",
       "      <td>▁공</td>\n",
       "      <td>중에</td>\n",
       "      <td>▁수직</td>\n",
       "      <td>의</td>\n",
       "      <td>▁파</td>\n",
       "      <td>문을</td>\n",
       "      <td>▁내</td>\n",
       "      <td>이며</td>\n",
       "      <td>▁고요</td>\n",
       "      <td>히</td>\n",
       "      <td>▁떨어지는</td>\n",
       "      <td>▁오</td>\n",
       "      <td>동</td>\n",
       "      <td>잎</td>\n",
       "      <td>은</td>\n",
       "      <td>▁누구</td>\n",
       "      <td>의</td>\n",
       "      <td>▁발자</td>\n",
       "      <td>취</td>\n",
       "      <td>▁입</td>\n",
       "      <td>니까.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input_IDs</th>\n",
       "      <td>31140</td>\n",
       "      <td>20780</td>\n",
       "      <td>30359</td>\n",
       "      <td>30016</td>\n",
       "      <td>31373</td>\n",
       "      <td>41427</td>\n",
       "      <td>25792</td>\n",
       "      <td>30163</td>\n",
       "      <td>31047</td>\n",
       "      <td>30024</td>\n",
       "      <td>31111</td>\n",
       "      <td>51068</td>\n",
       "      <td>29936</td>\n",
       "      <td>36152</td>\n",
       "      <td>30027</td>\n",
       "      <td>20801</td>\n",
       "      <td>25846</td>\n",
       "      <td>25768</td>\n",
       "      <td>31199</td>\n",
       "      <td>25792</td>\n",
       "      <td>44202</td>\n",
       "      <td>27472</td>\n",
       "      <td>30148</td>\n",
       "      <td>37708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0      1      2      3      4      5      6      7      8   \\\n",
       "kogpt-2_tokens    ▁바람      도    ▁없는     ▁공     중에    ▁수직      의     ▁파     문을   \n",
       "Input_IDs       31140  20780  30359  30016  31373  41427  25792  30163  31047   \n",
       "\n",
       "                   9      10     11     12     13     14     15     16     17  \\\n",
       "kogpt-2_tokens     ▁내     이며    ▁고요      히  ▁떨어지는     ▁오      동      잎      은   \n",
       "Input_IDs       30024  31111  51068  29936  36152  30027  20801  25846  25768   \n",
       "\n",
       "                   18     19     20     21     22     23  \n",
       "kogpt-2_tokens    ▁누구      의    ▁발자      취     ▁입    니까.  \n",
       "Input_IDs       31199  25792  44202  27472  30148  37708  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_txt = \"바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.\"\n",
    "tokens = tokenizer(input_txt).tokens()\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].numpy()\n",
    "pd.options.display.max_columns = 40\n",
    "pd.options.display.max_rows = 60\n",
    "df = pd.DataFrame([tokens, input_ids[0]], index=[\"kogpt-2_tokens\", \"Input_IDs\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c349dc",
   "metadata": {},
   "source": [
    "디코딩 방식 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e09a1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_length=128\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef11823",
   "metadata": {},
   "source": [
    "출력 결과에서 공백/빈줄이 나오는 이유? <br>\n",
    "do_sample=False로 설정하면 모델이 가장 높은 확률의 토큰을 계속 선택하여 공백, 빈줄이 반복해서 나옴 -> **top-k 샘플링이나 top-p 샘플링**과 같은 **확률적 생성 방식**을 사용하여 다양한 토큰이 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7248cf",
   "metadata": {},
   "source": [
    "#### top-k 샘플링 vs top-p 샘플링\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6756baa5",
   "metadata": {},
   "source": [
    "빔 서치 디코딩을 사용하고 n-gram 패널티까지 부과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d54c8fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.</d>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=10, no_repeat_ngram_size=2,\n",
    "                             do_sample=False)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da120dd",
   "metadata": {},
   "source": [
    "샘플링 기법 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3f9878",
   "metadata": {},
   "source": [
    "Temperature 샘플링<br>\n",
    "temperature는 확률 분포를 조절하여 텍스트의 다양성을 조정하는 매개변수입니다.<br>\n",
    "\n",
    "temperature=1.0이면 원래 확률을 그대로 사용하지만, temperature<1일 때는 확률이 높은 단어가 더욱 강화되고, temperature>1일 때는 확률이 낮은 단어의 선택 가능성이 높아집니다.\n",
    "<br>\n",
    "예시: temperature=2.0으로 설정하면, 확률이 높은 단어 A의 확률이 상대적으로 낮아지고, 낮은 확률의 단어들도 선택 가능성이 높아집니다. 결과적으로 모델이 더 창의적인 선택을 하게 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad4dd0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까. 나는 그 발자국 소리에 귀기울입니다. 산새의 노랫소리도 귓전에 들립니다. 바람과 구름과 꽃의 속삭임에도 나는 가슴이 뜁니다. 내 생애 단 한번뿐인 사랑하는 이에게 줄 꽃 한 송이 꽃봉오리를 맺는 동안에도 내 눈에서는 오직 당신만 빛납니다. 당신만이 내 삶의 빛이고 행복입니다. \n",
      " \n",
      " - 오정방 전남 강진에서 태어난 시인. 광주고와 전남대를 졸업했다. 1963년 조선일보 신춘문예에 당선돼 등단했다.\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2,\n",
    "                             do_sample=True, temperature=2.0, top_k=50)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adbacbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까. \n",
      " \n",
      " ------------------------------------------------ \n",
      " 오월의 마지막 날. \n",
      " 이 시를 읽으며 \n",
      " 내 마음도 \n",
      " 한없이 \n",
      " 고요해지고 \n",
      " 마음 한구석에 \n",
      " 아련한 그리움이 피어오른다.</d>\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=10, no_repeat_ngram_size=2,\n",
    "                             do_sample=True, top_p=0.1)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad835bcb",
   "metadata": {},
   "source": [
    "### 오류 발생\n",
    "원문이 그대로만 출력됨. 문제가 무엇일까?? <br>\n",
    "num_beams도 7->10개로 트리처럼 여러 단어를 예측하게 수정해보았고, top_p=0.9->0.99까지 더 많은 단어를 포함시키게 하였다 -> 출력 변화 없음.<br>\n",
    "<br>\n",
    "오히려, top_p를 0.1로 낮추자 여러 단어가 출력되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4814ea90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까. \n",
      " \n",
      " <unk><unk>시집<unk>, <마음>, 문학과지성사, 1996, 시집.</d>\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(\n",
    "    input_ids, \n",
    "    max_length=max_length, \n",
    "    num_beams=1, \n",
    "    no_repeat_ngram_size=2, \n",
    "    do_sample=True, \n",
    "    temperature=1.5, \n",
    "    top_k=80, \n",
    "    top_p=0.1\n",
    ")\n",
    "print(tokenizer.decode(output_beam[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda7c760",
   "metadata": {},
   "source": [
    "## 2. 데이터셋 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b4a320",
   "metadata": {},
   "source": [
    "### 1) SFT 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc63101c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">json</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>data_path_1_SFT = <span style=\"color: #808000; text-decoration-color: #808000\">'~/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl'</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">open</span>(data_path_1_SFT, <span style=\"color: #808000; text-decoration-color: #808000\">\"r\"</span>, encoding=<span style=\"color: #808000; text-decoration-color: #808000\">'utf-8-sig'</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> json_file:                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>list_data_dict = json.load(json_file)                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(list_data_dict))                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">FileNotFoundError: </span><span style=\"font-weight: bold\">[</span>Errno <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span> No such file or directory: <span style=\"color: #008000; text-decoration-color: #008000\">'~/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m3\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1 \u001b[0m\u001b[94mimport\u001b[0m \u001b[4;96mjson\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0mdata_path_1_SFT = \u001b[33m'\u001b[0m\u001b[33m~/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl\u001b[0m\u001b[33m'\u001b[0m                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3 \u001b[94mwith\u001b[0m \u001b[96mopen\u001b[0m(data_path_1_SFT, \u001b[33m\"\u001b[0m\u001b[33mr\u001b[0m\u001b[33m\"\u001b[0m, encoding=\u001b[33m'\u001b[0m\u001b[33mutf-8-sig\u001b[0m\u001b[33m'\u001b[0m) \u001b[94mas\u001b[0m json_file:                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m\u001b[2m│   \u001b[0mlist_data_dict = json.load(json_file)                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m5 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m6 \u001b[0m\u001b[96mprint\u001b[0m(\u001b[96mlen\u001b[0m(list_data_dict))                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mFileNotFoundError: \u001b[0m\u001b[1m[\u001b[0mErrno \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m No such file or directory: \u001b[32m'~/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json \n",
    "data_path_1_SFT = '~/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl' \n",
    "with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e6691b",
   "metadata": {},
   "source": [
    "**데이터는 질문과 답변 형식으로 존재**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd4ad17",
   "metadata": {},
   "source": [
    "### 2) RM 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6c8eb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span>data_path_2_RM = <span style=\"color: #808000; text-decoration-color: #808000\">'./aiffel/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl'</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">open</span>(data_path_2_RM, <span style=\"color: #808000; text-decoration-color: #808000\">\"r\"</span>, encoding=<span style=\"color: #808000; text-decoration-color: #808000\">'utf-8-sig'</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> json_file:                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>list_data_dict = json.load(json_file)                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(list_data_dict))                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">FileNotFoundError: </span><span style=\"font-weight: bold\">[</span>Errno <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span> No such file or directory: <span style=\"color: #008000; text-decoration-color: #008000\">'./aiffel/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m2\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1 \u001b[0mdata_path_2_RM = \u001b[33m'\u001b[0m\u001b[33m./aiffel/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl\u001b[0m\u001b[33m'\u001b[0m                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2 \u001b[94mwith\u001b[0m \u001b[96mopen\u001b[0m(data_path_2_RM, \u001b[33m\"\u001b[0m\u001b[33mr\u001b[0m\u001b[33m\"\u001b[0m, encoding=\u001b[33m'\u001b[0m\u001b[33mutf-8-sig\u001b[0m\u001b[33m'\u001b[0m) \u001b[94mas\u001b[0m json_file:                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m\u001b[2m│   \u001b[0mlist_data_dict = json.load(json_file)                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m5 \u001b[0m\u001b[96mprint\u001b[0m(\u001b[96mlen\u001b[0m(list_data_dict))                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mFileNotFoundError: \u001b[0m\u001b[1m[\u001b[0mErrno \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m No such file or directory: \u001b[32m'./aiffel/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path_2_RM = './aiffel/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl'\n",
    "with open(data_path_2_RM, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e669892e",
   "metadata": {},
   "source": [
    "**데이터는 질문과 답변 3개, 순위 형식으로 존재**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72359a7c",
   "metadata": {},
   "source": [
    "### 3) PPO 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92dd9734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span>data_path_3_PPO = <span style=\"color: #808000; text-decoration-color: #808000\">'./aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl'</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">open</span>(data_path_3_PPO, <span style=\"color: #808000; text-decoration-color: #808000\">\"r\"</span>, encoding=<span style=\"color: #808000; text-decoration-color: #808000\">'utf-8-sig'</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> json_file:                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>list_data_dict = json.load(json_file)                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(list_data_dict))                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">FileNotFoundError: </span><span style=\"font-weight: bold\">[</span>Errno <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span> No such file or directory: <span style=\"color: #008000; text-decoration-color: #008000\">'./aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m2\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1 \u001b[0mdata_path_3_PPO = \u001b[33m'\u001b[0m\u001b[33m./aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl\u001b[0m\u001b[33m'\u001b[0m                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2 \u001b[94mwith\u001b[0m \u001b[96mopen\u001b[0m(data_path_3_PPO, \u001b[33m\"\u001b[0m\u001b[33mr\u001b[0m\u001b[33m\"\u001b[0m, encoding=\u001b[33m'\u001b[0m\u001b[33mutf-8-sig\u001b[0m\u001b[33m'\u001b[0m) \u001b[94mas\u001b[0m json_file:                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m\u001b[2m│   \u001b[0mlist_data_dict = json.load(json_file)                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m5 \u001b[0m\u001b[96mprint\u001b[0m(\u001b[96mlen\u001b[0m(list_data_dict))                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mFileNotFoundError: \u001b[0m\u001b[1m[\u001b[0mErrno \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m No such file or directory: \u001b[32m'./aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path_3_PPO = './aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl'\n",
    "with open(data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5d7781",
   "metadata": {},
   "source": [
    "**AI가 답할만한 적절한 질문으로 존재**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b25ca1",
   "metadata": {},
   "source": [
    "## 3. Supervised Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c0fea",
   "metadata": {},
   "source": [
    "라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05fc2f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81d6c3",
   "metadata": {},
   "source": [
    "토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28363276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 모델과 토크나이저 교체\n",
    "model = AutoModelForCausalLM.from_pretrained('skt/ko-gpt-trinity-1.2B-v0.5')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/ko-gpt-trinity-1.2B-v0.5', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f43b67",
   "metadata": {},
   "source": [
    "모델 인퍼런스 단계(모델이 학습 후 새로운 데이터에 대해 예측하는 과정) 사용할 prompt 딕셔너리 템플릿과 SFT 데이터셋 클래스를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae369959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence #typing 모듈에서 옵션 타입, 딕셔너리 타입, 시퀀스 타입을 불러옵\n",
    "\n",
    "class SFT_dataset(Dataset): #PyTorch의 Dataset을 상속받아 모델 학습에 필요한 데이터셋을 관리\n",
    "\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        #SFT_dataset 클래스의 생성자 메서드입니다. 데이터 경로와 토크나이저를 인자로 받으며, 데이터셋 로딩과 전처리를 수행\n",
    "        super(SFT_dataset, self).__init__() #Dataset 클래스를 초기화\n",
    "        logging.warning(\"Loading data...\") #데이터셋 로딩 과정을 추적\n",
    "\n",
    "        pattern_instruction = 'prompt'  # instruction -> 데이터셋에서 명령어를 나타내는 키를 지정\n",
    "        pattern_output = 'completion'  # response -> 데이터셋에서 응답을 나타내는 키를 지정\n",
    "\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file: # 지정된 경로의 JSON 파일을 열고, 이를 파싱하여 리스트 형식의 데이터 딕셔너리로 부름\n",
    "            list_data_dict = json.load(json_file) # list_data_dict에는 여러 개의 예시 데이터가 저장\n",
    "\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\" #명령어와 응답 형식을 지정한 템플릿 딕셔너리를 정의\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            tmp = prompt_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dcdf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object): \n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b910b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SFT_dataset(data_path_1_SFT='./aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl', tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "print('input : %s'%train_dataset.input_ids[0])\n",
    "print('output: %s'%train_dataset.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1cc164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.input_ids[0]를 디코딩해보세요.\n",
    "def decode_tokens(dataset, tokenizer, index=0):\n",
    "    # input_ids와 labels를 디코딩하여 원래 문장으로 변환\n",
    "    input_text = tokenizer.decode(dataset.input_ids[index], skip_special_tokens=True)\n",
    "    label_text = tokenizer.decode([token if token != -100 else tokenizer.pad_token_id for token in dataset.labels[index]], skip_special_tokens=True)\n",
    "    \n",
    "    return input_text, label_text\n",
    "\n",
    "# 예시 사용법\n",
    "input_text, label_text = decode_tokens(train_dataset, tokenizer)\n",
    "print(\"Decoded Input:\", input_text)\n",
    "print(\"Decoded Label:\", label_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30935a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/aiffel/KoChatGPT/test\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16 = True\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e4c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained('/aiffel/KoChatGPT/output_1_SFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb14c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
